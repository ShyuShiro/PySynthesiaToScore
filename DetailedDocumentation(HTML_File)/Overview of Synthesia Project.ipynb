{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to provide an overview on the Synthesia project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from IPython.display import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Project Preface](#first)\n",
    "\n",
    "Package usage:\n",
    "- Pandas (For organization in dataframes)\n",
    "- Numpy (For math operations such as rounding and ceiling)\n",
    "- Collections, os, and IPython for small features and convenience\n",
    "- LilyPond \\[GNU license\\] (To write music sheets from text)\n",
    "- Mingus \\[GNU License\\] (To talk to LilyPond and create the .ly file LilyPond needs)\n",
    "- cv2 \\[BSD License\\] -- Opencv (To analyze the video feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Intro & Goal](#second)\n",
    "\n",
    "## Project Goal\n",
    "\n",
    "In this project I hope to convert a Synthesia video into a printable music sheet!\n",
    "\n",
    "I achieved this through video processing (opencv2) to detect pixel movement, convert pixels to string (notes), and string to musical notes (mingus).\n",
    "\n",
    "![title](ProjectGoal.png)\n",
    "\n",
    "## Why this approach? Why video processing?\n",
    "\n",
    "There are libraries such as `Librosa` which process mp3 files as time-series data much like you would see in applications like `Audacity`. This would be the more logical approach as it would likely involve less noise, probably easier to convert to a music sheet, and works on all videos you find online (where as my script only works on specifically Synthesia videos).\n",
    "\n",
    "However to learn more and expand my knowledge, I've never worked with videos before; at best, only images. Doing this project has been a big learning experience and I'm glad I did it the way I did.\n",
    "\n",
    "In the future though, It'd be great to revisit this project with mp3 as the approach as getting an mp3 file is easier to aquire and thus generally more useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [OpenCV](#third)\n",
    "\n",
    "## Obtaining video feed and masking using OpenCV2\n",
    "\n",
    "Sentdex (who runs https://pythonprogramming.net/) has a youtube playlist that has helped greatly and I would recommend their tutorials if you're looking to learn OpenCV2!\n",
    "- https://www.youtube.com/watch?v=Z78zbnLlPUA&list=PLQVvvaa0QuDdttJXlLtAJxJetJcqmqlQq) \n",
    "\n",
    "\n",
    "Using a mask, I created blue and green masks of the original video feed based on HSV (Hue, Saturation, Value) colors. Utilization of BGR (Default OpenCV2 instead of RGB) is difficult for precise differentiation of differing hues of 1 color (Ex: Light blue vs Dark blue)\n",
    "\n",
    "![title](Masking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masks were created by [lower,upper] bound HSV values for green and then blue colors utilizing standard Green and Blue values as the starting ground (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Green in HSV:  [[[ 60 255 255]]]\n",
      "Blue in HSV:  [[[120 255 255]]]\n"
     ]
    }
   ],
   "source": [
    "#Utilize BGR scale to create pure green and blue colors\n",
    "green = np.uint8([[[0,255,0 ]]])\n",
    "blue = np.uint8([[[255,0,0 ]]])\n",
    "\n",
    "#Convert to HSV scale\n",
    "hsv_green = cv2.cvtColor(green,cv2.COLOR_BGR2HSV)\n",
    "hsv_blue = cv2.cvtColor(blue,cv2.COLOR_BGR2HSV)\n",
    "\n",
    "#Printout the HSV version of pure green and pure blue\n",
    "print( \"Green in HSV: \",hsv_green )\n",
    "print( \"Blue in HSV: \",hsv_blue )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel detection\n",
    "\n",
    "First I created cropped 1-px tall region (indicated by the red bounding box in the below image) which is ($w$,1) pixels in dimension where $w$ is the width of the video (640 px)\n",
    "\n",
    "![title](Crop_region.png)\n",
    "\n",
    "Below is a summary of the code used to set up the bounding box (`cropped`) to look for green (`grn_mask_crop`) and blue (`blu_mask_crop`) notes within the bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "outputs": [],
   "source": [
    "cropped = frame[start:start+1, 0:w] #Start = 250 == A random height at which to draw the bounding box\n",
    "hsv_crop = cv2.cvtColor(cropped,cv2.COLOR_BGR2HSV) #Convert bounding box to HSV scale\n",
    "\n",
    "    #Green filter\n",
    "    grn_lower = np.array([30,90,100]) #Lower bound of \"Green\" color\n",
    "    grn_upper = np.array([80,255,255]) #upr bound\n",
    "    grn_mask_crop = cv2.inRange(hsv_crop,grn_lower,grn_upper) #Taking the bounding box... Find only green notes\n",
    "    \n",
    "    #Blue filter\n",
    "    blu_lower = np.array([75,20,120])\n",
    "    blu_upper = np.array([140,230,250])\n",
    "    blu_mask_crop = cv2.inRange(hsv_crop,blu_lower,blu_upper) #\"     \" Find only blue notes\n",
    "\n",
    "#These video feeds are the ones shown in the above images with only green or blue notes --- They serve for debug/visual\n",
    "    #interpretation purposes only -- They do not contribute any usefulness to the bounding box detection code\n",
    "hsv = cv2.cvtColor(frame,cv2.COLOR_BGR2HSV) #Convert entire video feed to HSV scale\n",
    "grn_mask = cv2.inRange(hsv,grn_lower,grn_upper) #Mask the HSV feed to filter for only green (Visual purposes only)\n",
    "blu_mask = cv2.inRange(hsv,blu_lower,blu_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually detect the notes I used `cv2.findNonZero(grn_mask_crop)` to detect any pixels that aren't black in the bounding box --- In this case, any green notes in the bounded region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_right=cv2.findNonZero(grn_mask_crop) #Pixel locations of where there are green\n",
    "coord_left=cv2.findNonZero(blu_mask_crop) #Pixel locations of where there are blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These coordinate outputs have the following format, below is shown for a pure \"G\" note located on the 3rd octave:\n",
    "\n",
    "\\>> coord_right <br>\n",
    "[ [ 246 , 0 ]<br>\n",
    "  [ 247 , 0 ]<br>\n",
    "  [ 248 , 0 ]<br>\n",
    "  [ 249 , 0 ]<br>\n",
    "  [ 250 , 0 ]<br>\n",
    "  [ 251 , 0 ]<br>\n",
    "  [ 252 , 0 ]<br>\n",
    "  [ 253 , 0 ]<br>\n",
    "  [ 254 , 0 ]<br>\n",
    "  [ 255 , 0 ]<br>\n",
    "  [ 256 , 0 ]<br>\n",
    "  [ 257 , 0 ] ]\n",
    "  \n",
    "Note above is a pure \"G-3\"\n",
    "\n",
    "So great! We have pixels, but we want notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Pixel to Notes](#fourth)\n",
    "\n",
    "## Counting pixels ... literally \n",
    "Utilizing MSpaint or your favorite image editing software such as Photoshop, you can experience the joy of meticulously counting pixels that align to each and every 86 keys of the piano keyboard!\n",
    "\n",
    "![PixelMeasuring](PixelMeasuring.png)\n",
    "\n",
    "Here is the pixel alignment measurement I did for middle C (C-4).\n",
    "\n",
    "As you'll note in the image, I have \"Old\" and \"New\" values that I gave to the pixels -- Originally I thought maybe it would be smart to prioritize pixel-real-estate to the white keys because they're bigger. This however yielded many false detections, especially false negatives among the black keys.\n",
    "\n",
    "By changing and updating to the new values, I gave each key an equal voice which aided in my noise-filtering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the middle-C octave (you'll note that Synthesia calls middle C \"C3\" but anyone else in the internet calls it \"C4\" as far as I've been trained) I gathered that an octave is 86-px in length, so I simply duplicated this 1 octave measurement up and down the pixel map until I had a map for pixel 0 --> pixel 640 (width of the video feed)\n",
    "\n",
    "## Mapping pixels to Notes\n",
    "\n",
    "Now that I had the pixel --> Note for each pixel in the video feed\n",
    "\n",
    "- 1) I created a dictionary to map pixels to notes called `ConvertPixelToNote(val)` which converted single pixels to the note in that range of pixel locations.\n",
    "- 2) Amplified the mapping to list objects such as those from the bounding box detection and wrapped it into the function  `ConvertListToNote(lst,vote_threshold=5,percent_threshold=0.2)`\n",
    "\n",
    "Below are the functions themselves -- Beware, the PixelToNote mapping is a vary tall function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     5,
     37,
     62,
     87,
     112,
     137,
     162
    ]
   },
   "outputs": [],
   "source": [
    "#Define custom functions for Pixel <-> Note conversion\n",
    "def ConvertPixelToNote(val):\n",
    "    '''\n",
    "    Converts single pixel into note\n",
    "    '''\n",
    "    if 0 <= val <= 24: #Octave 0\n",
    "        if 0 <= val <= 9:\n",
    "            note = \"A-0\"\n",
    "        elif 10 <= val <= 16:\n",
    "            note = \"A#-0\"\n",
    "        elif 17 <= val <= 24:\n",
    "            note = \"B-0\"\n",
    "    elif 25 <= val <= 111: #Octave 1\n",
    "        if 25 <= val <= 31:\n",
    "            note = \"C-1\"\n",
    "        elif 32 <= val <= 38:\n",
    "            note = \"C#-1\"\n",
    "        elif 39 <= val <= 45:\n",
    "            note = \"D-1\"\n",
    "        elif 46 <= val <= 52:\n",
    "            note = \"D#-1\"\n",
    "        elif 53 <= val <= 60:\n",
    "            note = \"E-1\"\n",
    "        elif 61 <= val <= 67:\n",
    "            note = \"F-1\"\n",
    "        elif 68 <= val <= 74:\n",
    "            note = \"F#-1\"\n",
    "        elif 75 <= val <= 81:\n",
    "            note = \"G-1\"\n",
    "        elif 82 <= val <= 88:\n",
    "            note = \"G#-1\"\n",
    "        elif 89 <= val <= 95:\n",
    "            note = \"A-1\"\n",
    "        elif 96 <= val <= 102:\n",
    "            note = \"A#-1\"\n",
    "        elif 103 <= val <= 111:\n",
    "            note = \"B-1\"\n",
    "    elif 112 <= val <= 197: #Octave 2\n",
    "        if 112 <= val <= 118:\n",
    "            note = \"C-2\"\n",
    "        elif 119 <= val <= 125:\n",
    "            note = \"C#-2\"\n",
    "        elif 126 <= val <= 132:\n",
    "            note = \"D-2\"\n",
    "        elif 133 <= val <= 139:\n",
    "            note = \"D#-2\"\n",
    "        elif 140 <= val <= 147:\n",
    "            note = \"E-2\"\n",
    "        elif 148 <= val <= 154:\n",
    "            note = \"F-2\"\n",
    "        elif 155 <= val <= 161:\n",
    "            note = \"F#-2\"\n",
    "        elif 162 <= val <= 168:\n",
    "            note = \"G-2\"\n",
    "        elif 169 <= val <= 175:\n",
    "            note = \"G#-2\"\n",
    "        elif 176 <= val <= 182:\n",
    "            note = \"A-2\"\n",
    "        elif 183 <= val <= 189:\n",
    "            note = \"A#-2\"\n",
    "        elif 190 <= val <= 197:\n",
    "            note = \"B-2\"        \n",
    "    elif 198 <= val <= 283: #Octave 3\n",
    "        if 198 <= val <= 204:\n",
    "            note = \"C-3\"\n",
    "        elif 205 <= val <= 211:\n",
    "            note = \"C#-3\"\n",
    "        elif 212 <= val <= 218:\n",
    "            note = \"D-3\"\n",
    "        elif 219 <= val <= 225:\n",
    "            note = \"D#-3\"\n",
    "        elif 226 <= val <= 233:\n",
    "            note = \"E-3\"\n",
    "        elif 234 <= val <= 240:\n",
    "            note = \"F-3\"\n",
    "        elif 241 <= val <= 247:\n",
    "            note = \"F#-3\"\n",
    "        elif 248 <= val <= 254:\n",
    "            note = \"G-3\"\n",
    "        elif 255 <= val <= 261:\n",
    "            note = \"G#-3\"\n",
    "        elif 262 <= val <= 268:\n",
    "            note = \"A-3\"\n",
    "        elif 269 <= val <= 275:\n",
    "            note = \"A#-3\"\n",
    "        elif 276 <= val <= 283:\n",
    "            note = \"B-3\"\n",
    "    elif 284 <= val <= 369: #Octave 4\n",
    "        if 284 <= val <= 290:\n",
    "            note = \"C-4\"\n",
    "        elif 291 <= val <= 297:\n",
    "            note = \"C#-4\"\n",
    "        elif 298 <= val <= 304:\n",
    "            note = \"D-4\"\n",
    "        elif 305 <= val <= 311:\n",
    "            note = \"D#-4\"\n",
    "        elif 312 <= val <= 319:\n",
    "            note = \"E-4\"\n",
    "        elif 320 <= val <= 326:\n",
    "            note = \"F-4\"\n",
    "        elif 327 <= val <= 333:\n",
    "            note = \"F#-4\"\n",
    "        elif 334 <= val <= 340:\n",
    "            note = \"G-4\"\n",
    "        elif 341 <= val <= 347:\n",
    "            note = \"G#-4\"\n",
    "        elif 348 <= val <= 354:\n",
    "            note = \"A-4\"\n",
    "        elif 355 <= val <= 361:\n",
    "            note = \"A#-4\"\n",
    "        elif 362 <= val <= 369:\n",
    "            note = \"B-4\"\n",
    "    elif 370 <= val <= 455: #Octave 5\n",
    "        if 370 <= val <= 376:\n",
    "            note = \"C-5\"\n",
    "        elif 377 <= val <= 383:\n",
    "            note = \"C#-5\"\n",
    "        elif 384 <= val <= 390:\n",
    "            note = \"D-5\"\n",
    "        elif 391 <= val <= 397:\n",
    "            note = \"D#-5\"\n",
    "        elif 398 <= val <= 405:\n",
    "            note = \"E-5\"\n",
    "        elif 406 <= val <= 412:\n",
    "            note = \"F-5\"\n",
    "        elif 413 <= val <= 419:\n",
    "            note = \"F#-5\"\n",
    "        elif 420 <= val <= 426:\n",
    "            note = \"G-5\"\n",
    "        elif 427 <= val <= 433:\n",
    "            note = \"G#-5\"\n",
    "        elif 434 <= val <= 440:\n",
    "            note = \"A-5\"\n",
    "        elif 441 <= val <= 447:\n",
    "            note = \"A#-5\"\n",
    "        elif 448 <= val <= 455:\n",
    "            note = \"B-5\"\n",
    "    elif 456 <= val <= 541: #Octave 6\n",
    "        if 456 <= val <= 462:\n",
    "            note = \"C-6\"\n",
    "        elif 463 <= val <= 469:\n",
    "            note = \"C#-6\"\n",
    "        elif 470 <= val <= 476:\n",
    "            note = \"D-6\"\n",
    "        elif 477 <= val <= 483:\n",
    "            note = \"D#-6\"\n",
    "        elif 484 <= val <= 491:\n",
    "            note = \"E-6\"\n",
    "        elif 492 <= val <= 498:\n",
    "            note = \"F-6\"\n",
    "        elif 499 <= val <= 505:\n",
    "            note = \"F#-6\"\n",
    "        elif 506 <= val <= 512:\n",
    "            note = \"G-6\"\n",
    "        elif 513 <= val <= 519:\n",
    "            note = \"G#-6\"\n",
    "        elif 520 <= val <= 526:\n",
    "            note = \"A-6\"\n",
    "        elif 527 <= val <= 533:\n",
    "            note = \"A#-6\"\n",
    "        elif 534 <= val <= 541:\n",
    "            note = \"B-6\"\n",
    "    elif 542 <= val <= 640: #Octave 7 & 8\n",
    "        if 542 <= val <= 548:\n",
    "            note = \"C-7\"\n",
    "        elif 549 <= val <= 555:\n",
    "            note = \"C#-7\"\n",
    "        elif 556 <= val <= 562:\n",
    "            note = \"D-7\"\n",
    "        elif 563 <= val <= 569:\n",
    "            note = \"D#-7\"\n",
    "        elif 570 <= val <= 577:\n",
    "            note = \"E-7\"\n",
    "        elif 578 <= val <= 584:\n",
    "            note = \"F-7\"\n",
    "        elif 585 <= val <= 591:\n",
    "            note = \"F#-7\"\n",
    "        elif 592 <= val <= 598:\n",
    "            note = \"G-7\"\n",
    "        elif 599 <= val <= 605:\n",
    "            note = \"G#-7\"\n",
    "        elif 606 <= val <= 612:\n",
    "            note = \"A-7\"\n",
    "        elif 613 <= val <= 619:\n",
    "            note = \"A#-7\"\n",
    "        elif 620 <= val <= 628:\n",
    "            note = \"B-7\" \n",
    "        elif 629 <= val <= 640:\n",
    "            note = \"C-8\" \n",
    "    try:\n",
    "        return note #Return the note if there is something\n",
    "    except:\n",
    "        note = None\n",
    "\n",
    "#Step up ConvertPixelToNote to work for a list object & apply Collaborative Voting        \n",
    "def ConvertListToNote(lst,vote_threshold=5,percent_threshold=0.20,debug=False):\n",
    "    '''\n",
    "    Uses ConvertPixelToNote to converts list object to note(s) based on collaborative voting\n",
    "    '''\n",
    "    temp = Counter()\n",
    "    for val in lst: #Read all pixels, convert to notes, and increment counts\n",
    "        converted_note = ConvertPixelToNote(val)\n",
    "        if converted_note != None: #Do not count \"None\" as a note\n",
    "            temp[converted_note] += 1\n",
    "    #Stage 1 elimination\n",
    "    #Eliminate smaller noise so that they don't contribute to the overall percentage\n",
    "    keys = list(temp.keys())\n",
    "    backup = temp\n",
    "    for note in keys:\n",
    "        if temp[note] <= vote_threshold:\n",
    "            temp.pop(note)\n",
    "    \n",
    "    #Stage 2 elimination -- Now compute total_hits after first elimination\n",
    "    total_hits = np.sum(list(temp.values()))\n",
    "    if debug==True:\n",
    "        print(\"Keys before stage 1 elimination: \",backup,\n",
    "              \"\\nKeys going into stage 2: \",temp,\"\\nTotal Hits: \",total_hits)\n",
    "    for note in list(temp.keys()): #For each note detected\n",
    "        #Want to keep majority by count or percentage\n",
    "        #Remove notes that don't occur often or are not majority %'age\n",
    "        if temp[note]/total_hits<=percent_threshold:\n",
    "            temp.pop(note)\n",
    "    return list(temp.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were paying attention you may have noticed some arguements in the last function\n",
    "\n",
    "what are \"vote_threshold\" and \"percent_threshold\"? These are my hyperparameters for my filtering algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering out noise -- False positive and False negative handling\n",
    "\n",
    "### Approach 1 - Take the median\n",
    "The approach I've taken to analyze video feeds instead of raw mp3 audio as an input is inherently noisy. There are bound to be misreads.\n",
    "\n",
    "Originally I took the simple approach of \"find the median pixel ---> That is your note\"\n",
    "\n",
    "For simple cases such as the pure G-3 detection provided earlier ... the median pixel would be `251` which based on the mapping would churn out `G-3` ... perfect!\n",
    "\n",
    "However this approach falls apart when you're given chords such as a ['G-2','G-3'] octave which would produce `C#-3'\n",
    "\n",
    "-- There needed to be a better aprroach that could not only handle multiple notes, but also filter out the noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2 - Collaborative Voting\n",
    "Notes in Synthesia bleed into neighboring notes' registries. Seen below I used the marque tool to draw a dotted rectangle around the `G-3` and `A-3` notes coming down to the keyboard.\n",
    "\n",
    "![BleedingNotes](BleedingNotes.png)\n",
    "\n",
    "You'll notice\n",
    "- `G-3` bleeds into `F#-3` a little and quite heavily into `G#-3`\n",
    "- `A-3` bleeds into `A#-3` a little and quite heavily into `G#-3`\n",
    "\n",
    "This has 2 implications...\n",
    "\n",
    "- 1) There needs to be a 'voting' system which can filter out the false-negative detection of neighboring notes\n",
    "- 2) If a ['G-3','A-3'] note or any 2nd interval chord appears, it will completely cover [`G-3`,`G#-3`,`A-3`] and be impossible to resolve!\n",
    "\n",
    "To apply this 'voting' system I had a 2-stage elimination process\n",
    "\n",
    "- 1) Eliminate false detections if they don't have enough of their pixels firing (Less than or equal to 5 counts)\n",
    "- 2) Eliminate the remaining notes if they don't contribute to the majority vote (less than or equal to 20% of the overall detections)\n",
    "\n",
    "Refer to `ConvertListToNote()` function above to see the stage-1 and stage-2 elimination code. <br>\n",
    "An example of the Collaborative Voting in action is provided below:\n",
    "\n",
    "![CollaborativeVoting](CollaborativeVoting3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reductions in noise\n",
    "\n",
    "1) Read every other frame\n",
    "\n",
    "This was done to reduce computation (which I thought would be a problem but this code moves quite fast) as well as reduce the chances of the below detection cases where the next note may bleed into the bounding box before the current note is finished\n",
    "\n",
    "![EveryOtherFrame](NoisyDetection.png)\n",
    "\n",
    "2) Only record notes if they are different from the previously registered note\n",
    "\n",
    "Previously I was recording information in the following format\n",
    "\n",
    "| Frame | 150 | 151 | 152 | 153 | 154 | 155  | 156  | 157  |   |\n",
    "|-------|-----|-----|-----|-----|-----|------|------|------|---|\n",
    "| Note  | A-3 | A-3 | A-3 | A-3 | A-3 | F#-3 | F#-3 | F#-3 |   |\n",
    "\n",
    "Note duration (ie: how long a note should be played for) is important for musical compositions .. but I decided to measure how long a note is based on the time difference to the next note being played.\n",
    "\n",
    "Thus, I don't need each frame that a note is sustained for -- just when it begins and when the next note begins to yield the equivalent below table:\n",
    "\n",
    "| Frame | 150 | 155  | \n",
    "|-------|-----|-----|\n",
    "| Note  | A-3 | F#-3 | \n",
    "\n",
    "Below is the code utilized to do that in summary:\n",
    "- 1) Detects that the bounding box detection `coord_right` found something in the box\n",
    "- 2) Converts the detection to a note\n",
    "- 3) If the note is not None (in case collaborative filtering has completely voted all detections out)\n",
    "- 3b) Append the note only if it wasn't the previous note\n",
    "- 3c) Or if `allow_duplicate_right/left`==True\n",
    "- 4) Last checkpoint ... sometimes the note returned would be [] and thus pass the \"None\" filter\n",
    "- 5) Append that note to the registry\n",
    "\n",
    "Comment on (3c). If a note is sustained or being repeated such as a repeating bass line in the left hand. I would want to capture each hit of that note! Using a boolean-lock, I was able to allow repeated notes by resetting the lock if the bounding box registered `None` suggesting there was a gap between the sustained note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Right hand detection\n",
    "if type(coord_right) == np.ndarray: #If not None basically\n",
    "    note_right = ConvertListToNote(coord_right[:,0][:,0]) #Convert the coordinates to a note\n",
    "    #Update only if there is a change to previously detected notes\n",
    "        #Determine if coord_right has read a \"None\" detection\n",
    "        #If so, then create a boolean lock to allow duplicate notes\n",
    "        #None detection denotes that there was a gap in note detection (Common in cases like repeated bass notes)\n",
    "    if note_right != None:\n",
    "        if note_right != notes_right[-1] or allow_duplicate_right==True: #If not the same as previously registered\n",
    "            #Register frames & notes to appropriate lists\n",
    "            if debug==True:\n",
    "                print(\"RH:\",current_frame,note_right)\n",
    "            if len(note_right) > 0: #If the note returned is not empty (sometimes a [] slips through, not sure how)\n",
    "                #Add to registry\n",
    "                frames_right.append(current_frame)\n",
    "                notes_right.append(note_right)\n",
    "                #print(\"RH:\",current_frame,note_right)\n",
    "                allow_duplicate_right = False #Disallow duplication of this held note\n",
    "else: #If previous reading was \"None\" (the only other reading type) allow duplicate notes\n",
    "    allow_duplicate_right = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure & Note Durations\n",
    "\n",
    "## Measure Detection\n",
    "\n",
    "Unfortunately this is going to be a small section.\n",
    "\n",
    "Synthesia has measure bars running down the video as thin white-lines which also have a number attached (Eg: 14) to them stating \"This is measure 14\".\n",
    "\n",
    "Trying to create a white-mask to identify measure bars crossing the bounding region did not work for me.\n",
    "\n",
    "Why? Probably because the video I downloaded was very low quality\n",
    "\n",
    "So why not download a better quality? I was afraid all the work I had put into the project would be wasted if I downloaded a new video (eg: What if it is no longer 640x360 format --> Redo all the pixel alignments)\n",
    "\n",
    "In the end, I simply detected notes and found the lowest frame at which notes were detected --- normalized everything to frame 0, and determined how long a frame was based on my intuition of the song/beats.\n",
    "\n",
    "Process:\n",
    "\n",
    "1) Find the frames on which notes occur <br>\n",
    "LH_frames = [147,160,180,205, ... ] <br>\n",
    "RH_frames = [155,170,189,194, ... ] \n",
    "\n",
    "2) Subtract the lowest number (147) from all frames <br>\n",
    "zeroed_LH_frames = [0,13,23,58, ... ] <br>\n",
    "zeroed_RH_frames = [8,23,42,47, ... ]\n",
    "\n",
    "3) __Hard-code__ in that a measure is ~58 frames (because based on the song, I know measure 2 begins on `D-3` which is played on that frame on the left-hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note Durations & Measure separation\n",
    "\n",
    "As mentioned, note durations are computed based on a \"time till the next note\" basis. Given the above \"normalized\" set of numbers, I go to further divide everything by the `measure_length` = 57.4\n",
    "   \n",
    ">Note: <br>I mentioned the measure is <span style=\"text-decoration: underline\">exactly</span> 58 frames from Measure 1 starting to Measure 2 starting. However in testing, the video starts to exhibit some measures of differing lengths. To capture the measure correctly, I found division by a number less than 58 yielded correct alignment of measures 1~16 and would correct itself by measure 18 ... leaving only Measure 17 to be incorrectly constructed (unfortunate). I'm sure this issue propregates further in the composition, but as I do not own the composition of the sheet im transcribing (the whole point of the project) I'm unable to determine when/which measures exactly are not correct\n",
    "\n",
    "normalized_LH_frames = [0,0.22,0.40,1.01, ... ] <br>\n",
    "normalized_RH_frames = [0.14,0.40,0.73,0.82, ... ]\n",
    "\n",
    "Utilizing these values, I applied a `np.ceil()` function to separate each note into their respective measures with the final dataframe in `pandas` looking as such:\n",
    "\n",
    "![dataframe](df_final.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note Resolution\n",
    "\n",
    "In the above image, the `Diff` column is the difference between the next frame and the current frame ... but in \"Note space\".\n",
    "\n",
    "$$ Diff = note\\_space\\{ Frame_{i+1} - Frame_{i} \\}$$\n",
    "\n",
    "Conversion to note space is done by dictionary mapping the time-space values to note-space values for Mingus/LilyPond consumption (more on that later).\n",
    "\n",
    "Computation:\n",
    "- 1) Take the \"Frame\" values and compute the difference as mentioned in the above equation\n",
    "- 2) Round the difference to the nearest 16th note (1/16 = 0.0625) value\n",
    "- 3) Convert rounded value to Note_space using Mingus\n",
    "\n",
    "| Frame | Difference | Rounded | Length in Note_space\n",
    "| ----- | ----- | ----- | ----- |\n",
    "|0.00 | 0.10 - 0.00 = 0.10 | 0.125 | 8 |\n",
    "|0.10 | 0.24 - 0.10 = 0.14 | 0.125 | 8 |\n",
    "|0.24 | 0.35 - 0.24 = 0.11 | 0.125 | 8 |\n",
    "|0.35 | 0.63 - 0.35 = 0.28 | 0.25 | 4 |\n",
    "|0.63 | --- | 0.125 | 8 |\n",
    "| ... | ... | ... | ... |\n",
    "\n",
    "How to round to an arbitrary number like the nearest 0.0625... divide the computation by that factor, round it, and multiply back that factor.\n",
    "\n",
    "$$ np.round(\\frac{frame_{i+1}-frame_{i}}{c})*c $$\n",
    "\n",
    "In the above equation $c = 0.0625 =$ the number you want to round to the nearest value of\n",
    "\n",
    "Conversion from decimal \"time space\" to \"Note_space\" was done via pandas mapping.\n",
    "\n",
    "... Basically what is happening is:\n",
    "- 1/8 = 0.125 --> Map 0.125 to \"8\" --> an eighth note in LilyPond syntax\n",
    "- 1/4 = 0.25 --> Map 0.25 to \"4\" --> a quarter note in LilyPond syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Last step -- Dictionary convert note length to note type (eigth/quarter etc)\n",
    "LengthToNote = dict({0.0625:16, #sixteenth\n",
    "                     0.125:8, #Eighth\n",
    "                     0.1875:8,\n",
    "                     0.25:4, #Quarter\n",
    "                     0.3125:4, #Round to dotted half\n",
    "                     0.375:value.dots(4), #Dotted quarter\n",
    "                     0.4375:value.dots(4), #Round to dotted half\n",
    "                     0.5:2, #Half\n",
    "                     0.5625:2, #Round to half\n",
    "                     0.625:2, #Round to half\n",
    "                     0.6875:2, #Round to half\n",
    "                     0.75:value.dots(2), #Dotted half\n",
    "                     0.8125:value.dots(2), #Round to dotted half\n",
    "                     0.875:value.dots(2), #Round to dotted half\n",
    "                     0.9375:value.dots(2), #Round to dotted half                \n",
    "                     1:1}) #Whole\n",
    "df[\"Diff\"] = df[\"Diff\"].map(LengthToNote) #Map to note values for Mingus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rest durations\n",
    "Among the notes, rests had to be determined as well otherwise. Any time the data frame had a blank space for \"Notes_l\" or \"Notes_r\" a rest note is placed with duration equal to the note being played on the opposite hand.\n",
    "\n",
    "Eg: In the first row of the above dataframe\n",
    "\n",
    "| Frame | Notes_l | Notes_r | Diff | Measure | Notes_both\n",
    "| - | - | - | - | - | - |\n",
    "|0.00 | [D-3] | | 8 | 1.0 | [D-3] |\n",
    "\n",
    "`Notes_r` is empty and thus a rest is placed in the right hand with value equal to that of the row --> eighth rest (Diff column reads 8) which is reflected in the music sheet as seen below.\n",
    "\n",
    "![FirstMeasures](Firstmeasures.png)\n",
    "\n",
    "This method of placing rests looks horrible because the sheet is now littered with rests of varying values. A musician may get mad that instead of defining 1xquarter rest my program spat out 2xeighth rests in measure 1 ... but I'm just looking forward to being able to play the song, so I didn't bother to create code to combine note values of similar notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mingus\n",
    "\n",
    "Mingus (https://bspaans.github.io/python-mingus/) is a python package that can link to LilyPond. LilyPond (https://lilypond.org/index.html) is a music engraving program which takes text an converts it to sheets or even midi. I won't go through how to use Mingus or Lilypond, but I'll take some time to talk about their nuances as they come up. \n",
    "\n",
    "To learn about Mingus\n",
    "- Here is a link to various tutorials as well as class structure of the different objects within Mingus: https://bspaans.github.io/python-mingus/\n",
    "\n",
    "To learn about LilyPond\n",
    "- Here is a basic intro to the script syntax: https://lilypond.org/text-input.html\n",
    "- Here are a multitude of tutorials/examples on what LilyPond can do: http://lilypond.org/doc/v2.19/Documentation/notation/index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Notes\n",
    "Utilizing Mingus's note strucutre .... \n",
    "\n",
    "I need to create a `Track` to place `Bars` (musical measures) on and `NoteContainers` to places on those `Bars`.\n",
    "\n",
    "^ This is the class structure of Mingus ^\n",
    "\n",
    "from `mingus.containers` import `NoteContainer`, `Bar`, and `Track`. I used `NoteContainer` instead of `Note` because the container can object can hold chords/intervals while the note object can only hold single notes (Not useful!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Measures & Track\n",
    "\n",
    "A musical `Track` is created separately for Left and Right hands. This was done because notes would otherwise always stem from the bass clef and attempt to draw from the bass clef even though it would make more sense to draw it on the treble clef.\n",
    "\n",
    "Example:\n",
    "![OutofScale](OutOfScaleNote.png)\n",
    "\n",
    "Once a track is created ... `Bar` objects are created sequentially for each existing measure\n",
    "\n",
    "The `NoteContainer` objects are placed within those bars based on the notes that exist in that measure\n",
    "\n",
    "Lastly, the `Bar` is appended to the `Track` and a LilyPond file (.ly) string is generated via `import mingus.extra.lilypond as LilyPond`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating tracks --- Left\n",
    "t_l = Track()\n",
    "for i in set(df[\"Measure\"].unique()[:-1]):\n",
    "    b = Bar(key=keysig)\n",
    "    subset = df[df[\"Measure\"]==i]\n",
    "    for j,k in zip(subset[\"Notes_l\"],subset[\"Diff\"]):\n",
    "        if len(j)>0: #If note is not NaN\n",
    "            nc = NoteContainer(j) #Define the note\n",
    "            b.place_notes(nc,k) #Add note to bar with length\n",
    "        else:\n",
    "            b.place_notes(None,k) #Place a rest note the lenght of the other hand's note\n",
    "    t_l + b\n",
    "LithHarbor_ly_left = LilyPond.from_Track(t_l) #Create .ly file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note1: The above code is only for the left hand ... similar code was copy pasted and used to generate the right hand track\n",
    "\n",
    "Note2: I should have called the .ly string a different name than `LithHarbor_ly_left` but ... the intent of the project was to only generate this specific song from the town of Maplestory called Lithharbor. So if you're using this code in the future --- that is your ly string object that LilyPond needs to generate its png/pdf files!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Music Sheets\n",
    "\n",
    "From the Left <ins>and</ins> Right .ly strings `LithHarbor_ly_left` and `LithHarbor_ly_right` a musical composition is generated into png or pdf via some stitching of strings.\n",
    "\n",
    "It isnt quite as simple as adding the 2 strings together in LilyPond syntax.\n",
    "\n",
    "- 1) Create a variable in LilyPond which I called \"rhMusic =\" for the right hand .ly string `LithHarbor_ly_right`\n",
    "- 2) Create a variable in LilyPond which I called \"lhMusic =\" for the left hand .ly string `LithHarbor_ly_left`\n",
    "- 3) Define a score\n",
    "- 4) Define the score to be a piano sheet (thus containing a bass and treble clef)\n",
    "- 5) Define the upper staff (\"RH\" by default LilyPond nomenclature) to be the `rhMusic` variable defined in Step #1\n",
    "- 6) Define the upper staff (\"LH\" by default LilyPond nomenclature) to be the `lhMusic` variable defined in Step #2\n",
    "- 6b) Additionally define this to bea \"\\\\clef bass\" just in case --- I actually never tested if the \\\\PianoStaff arguement handles that already\n",
    "- 7) Optional things were defined like the header object which contains the title and composer info\n",
    "- 8) Lastly the png/pdf is generated through `mingus.extra.lilpond.to_png({.ly string object},{Output name})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = '\\\\header { title = \"' + title + '\" composer = \"' + author + '\" opus = \"\" } '\n",
    "combine_test = header + \"rhMusic =  {\" + LithHarbor_ly_right + \"}\"\n",
    "combine_test = combine_test + \"lhMusic =  {\" + LithHarbor_ly_left + \"}\"\n",
    "combine_test = combine_test + \"\"\"\n",
    "\\\\score {\n",
    "  \\\\new PianoStaff <<\n",
    "    \\\\new Staff = \"RH\"  <<\n",
    "      \\\\rhMusic\n",
    "    >>\n",
    "    \\\\new Staff = \"LH\" <<\n",
    "      \\\\clef \"bass\"\n",
    "      \\\\lhMusic\n",
    "    >>\n",
    "  >>\n",
    "}\"\"\"\n",
    "\n",
    "LilyPond.to_png(combine_test, outputname) #Create png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Midi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a side note to the png/pdf file -- I wanted a .midi file to generate so I could \"test\" the output of the python script by having it play the png/pdf file back to me.\n",
    "\n",
    "A .midi file is generated from mingus through LilyPond via `from mingus.midi import midi_file_out as MidiFileOut\n",
    "`\n",
    "\n",
    "The code below will look <ins>extremely</ins> similar to the track generation code, but I combined both left and right hand notes into 1 single track object as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create midi file:\n",
    "#Combine Notes_r and Notes_l from the df into 1 congomerate\n",
    "combined_notes = []\n",
    "for i in range(df.shape[0]):\n",
    "    try:\n",
    "        combined_notes.append(df.iloc[i][\"Notes_l\"]+df.iloc[i][\"Notes_r\"])\n",
    "    except:\n",
    "        if df.iloc[i][\"Notes_l\"]!=\"\":\n",
    "            combined_notes.append(df.iloc[i][\"Notes_l\"])\n",
    "        else:\n",
    "            combined_notes.append(df.iloc[i][\"Notes_r\"])\n",
    "df[\"Notes_both\"] = combined_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the combined track info, I can generate the track and the subsequent midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now create a track by adding these notes from both hands\n",
    "t_both = Track()\n",
    "for i in set(df[\"Measure\"].unique()[:-1]):\n",
    "    b = Bar(key=keysig)\n",
    "    subset = df[df[\"Measure\"]==i]\n",
    "    for j,k in zip(subset[\"Notes_both\"],subset[\"Diff\"]):\n",
    "        if j: #If note is not NaN\n",
    "            nc = NoteContainer(j) #Define the note\n",
    "            b.place_notes(nc,k) #Add note to bar with length\n",
    "    t_both + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Product\n",
    "\n",
    "The final product of all this was a python script which ... unfortunately ... only works for this 1 specific video file.\n",
    "\n",
    "If you're reading this you'll be thinking -- Wow that sucks\n",
    "\n",
    "But that isnt the complete truth. This code can be adaptable to other synthesia .mp4 inputs by adjustment of the hyperparameters in the script\n",
    "\n",
    "\n",
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Hyperparameters\n",
    "debug = False #For output from Pixel <-> Note dictionaries\n",
    "vod_name = \"Above the Treetops - Lith Harbor Synthesia.mp4\" #Name of the file to read (.mp4!)\n",
    "fps = 1 #Speed of the video processing --- Inverse btw .. lower is faster!\n",
    "measure_length = 57.2 #The number of frames that yield a full measure\n",
    "start = 230 #Defining the height of the crop region\n",
    "keysig = \"D\" #Defining the key signature (in Major only!)\n",
    "bpm = 120 #Defining the BPM for the midi file -- Has no impact to png/pdf generation\n",
    "title = \"Above the Treetops - Lith Harbor\" #Title displayed above the score\n",
    "author = \"Leegle\" #Author displayed at the top right of the score\n",
    "outputname = \"LithHarbor\" #Name to use for all the output files ... eg: LithHarbor.png/.mid/.pdf\n",
    "#Mask filter parameters\n",
    "blu_lower = np.array([75,20,120]) #Lower bound of the Left hand\n",
    "blu_upper = np.array([140,230,250]) #Upper bound of the Left hand\n",
    "grn_lower = np.array([30,90,100]) #Lower bound of the Right hand\n",
    "grn_upper = np.array([80,255,255]) #Upper bound of the Right hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adjusting the parameters at the start of the python script, you can tune this script to work for whatever video file you want!\n",
    "\n",
    "- vod_name = Name of the video file you want to process\n",
    "- fps = processing speed of the video .. keep at 1. Increase to 10~60 as you want to debug and look into finer details\n",
    "- measure_length = How many frames you believe 1 measure of the musical score to be. If 58 frames, I recommend a value less like 57.2 as given in the example\n",
    "- start = the height at which the detection region is drawn --- Adjust to avoid any post-processing editing such as channel logos or text.\n",
    "- keysig = the key signature of the score. Only works for major keys, but im sure it can be fixed with a little more LilyPond work\n",
    "- bpm = Speed for the .midi file to play back at\n",
    "- title/author = the text to display on the png/pdf score\n",
    "- outputname = The name for which all the outputs will follow. Eg: \"LithHarbor.png\"\n",
    "- Mask parameters = Fine tuning to capture the different colors that the video will be playing back --- Lots and lots of Trial & Error!\n",
    "\n",
    "## What you need to do to work the script for your video file\n",
    "\n",
    "You'll have to adjust any parameters relating to your personal file such as the `vod_name` of course. \n",
    "\n",
    "But in terms of adjusting this script to your own needs:\n",
    "\n",
    "I suggest only adjusting `measure_length` for tuning parameters. If the video is using the default synthesia colors (Blue and Green) then you shouldn't have to adjust the mask parameters at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shortcomings\n",
    "Nothing is perfect. I only anticipated this project to spit output as string \"D-A-F#-E-F#-A\" for a single measure. This project has gone far beyond that initial scope and I couldn't be happier.\n",
    "\n",
    "## Measure Length\n",
    "Unless I had downloaded a better quality video output, I dont think OpenCV could capture the measure bar (thin white line) moving down the track. Because of this, I had to hard-code a feature to determine how long a measure is.\n",
    "\n",
    "This means 2 things:\n",
    "- 1) The code is less user-friendly & requires the user to put in quite a bit of leg work (Especially the leg work of knowing music enough to play the music in their own head & discern if that \"D-3\" should be in Measure 1 or if it should have been in Measure 2)\n",
    "- 2) The measure length seems to change throughout the video resulting in note cut off (because Mingus has a hard definition on measure length being 4 beats --- assuming 4/4 time signature)\n",
    "\n",
    "## Time signature is locked at 4/4\n",
    "This is a quick and easy fix to be quite honest, but my song was already in 4/4 and I didn't feel the need to code that into the `combined_test` string output.\n",
    "\n",
    "## Missing notes\n",
    "Kind of related to the measure length issue ... note would get chopped off.\n",
    "\n",
    "Ex: If I have 6 quarter notes in a row and 5 end up in measure 1. The 5th would disappear in the void. The 6th would appear as the first beat of measure 2. (assuming 4/4 time sig.)\n",
    "\n",
    "However on top of Mingus's hard definition on a measure resulting in void-notes. My pixel detection method is inherently noisy and I had to filter out the noise through the `collaborative voting` althorithm I created. Its not perfect, and thus some notes that should have existed don't make it through the filter.\n",
    "\n",
    "Lastly another point of missing notes is in the resolution of the note_duration mapping. I rounded all frame `Diff` counts to the nearest 0.0625 which is 1/16 = sixteenth note. That results in 2 things:\n",
    "- 1) 32nd notes can not be resolved/detected\n",
    "- 2) triplets or other syncopated notes were not coded for --- And thus may either register as incorrect beats or round to 0 and are deleted as a false detection\n",
    "\n",
    "Note that in case 2 --- The syncopated notes could then either shift more notes into that measure (which wouldnt result in voided notes, because I hard-sectioned off measures based on the `np.ceil()` function) or shift less notes into the measure and cut off the excess notes into the void because too many beats were placed into 1 single measure."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 cmd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "258px",
    "width": "470px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "406.875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
